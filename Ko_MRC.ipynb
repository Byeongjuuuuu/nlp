{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Byeongjuuuuu/nlp/blob/main/Ko_MRC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RgYksN3DtjPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef0d7d5a-3da5-4f6c-fbbb-079413805ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/Colab Notebooks/[GOORM]NLP/Project/Machine Reading Comprehension/7th-goorm-project-2-korean-mrc.zip'"
      ],
      "metadata": {
        "id": "9nS8RagxtkQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dc968dd-0870-4e34-b046-cca669a46b2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Colab Notebooks/[GOORM]NLP/Project/Machine Reading Comprehension/7th-goorm-project-2-korean-mrc.zip\n",
            "  inflating: baseline.csv            \n",
            "  inflating: blank.csv               \n",
            "  inflating: test.json               \n",
            "  inflating: train.json              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "8ZXSNg2LtkOW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43a9e31e-e787-4fa0-8995-fd23df60086b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.9.0-py3-none-any.whl (462 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (23.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m929.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.12.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2023.1.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, urllib3, multiprocess, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.9.0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.26.14 xxhash-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric#, list_metrics\n",
        "\n",
        "from transformers import AutoTokenizer, BertConfig, BertPreTrainedModel, AutoModelForQuestionAnswering\n",
        "from statistics import mean\n",
        "\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers.modeling_outputs import QuestionAnsweringModelOutput\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import warnings\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "metadata": {
        "id": "xKS0_u6gtkMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple, Dict, Any\n",
        "import json\n",
        "import random\n",
        "\n",
        "class KoMRC:\n",
        "    def __init__(self, data, indices: List[Tuple[int, int, int]]):\n",
        "        self._data = data\n",
        "        self._indices = indices\n",
        "\n",
        "    # Json을 불러오는 메소드\n",
        "    @classmethod\n",
        "    def load(cls, file_path: str):\n",
        "        with open(file_path, 'r', encoding='utf-8') as fd:\n",
        "            data = json.load(fd)\n",
        "\n",
        "        indices = []\n",
        "        for d_id, document in enumerate(data['data']):\n",
        "            for p_id, paragraph in enumerate(document['paragraphs']):\n",
        "                for q_id, _ in enumerate(paragraph['qas']):\n",
        "                    indices.append((d_id, p_id, q_id))\n",
        "        \n",
        "        return cls(data, indices)\n",
        "\n",
        "    # 데이터 셋을 잘라내는 메소드\n",
        "    @classmethod\n",
        "    def split(cls, dataset, eval_ratio: float=.1, seed=42):\n",
        "        indices = list(dataset._indices)\n",
        "        random.seed(seed)\n",
        "        random.shuffle(indices)\n",
        "        train_indices = indices[int(len(indices) * eval_ratio):]\n",
        "        eval_indices = indices[:int(len(indices) * eval_ratio)]\n",
        "\n",
        "        return cls(dataset._data, train_indices), cls(dataset._data, eval_indices)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Dict[str, Any]:\n",
        "        d_id, p_id, q_id = self._indices[index]\n",
        "        paragraph = self._data['data'][d_id]['paragraphs'][p_id]\n",
        "\n",
        "        \n",
        "        qa = paragraph['qas'][q_id]\n",
        "\n",
        "        guid = qa['guid']\n",
        "        question = qa['question']\n",
        "        answers = qa['answers'][0]\n",
        "        context = paragraph['context']\n",
        "        return {\n",
        "            'guid': guid,\n",
        "            'context': context,\n",
        "            'question': question,\n",
        "            'answers': answers\n",
        "        }\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._indices)\n"
      ],
      "metadata": {
        "id": "s_qN9sj2tkJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = KoMRC.load('./train.json')\n",
        "print(\"Number of Samples:\", len(dataset))\n",
        "print(dataset[0])"
      ],
      "metadata": {
        "id": "6C4YvPRTtkHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee948261-c018-480d-921f-10088659fb13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Samples: 12037\n",
            "{'guid': '798db07f0b9046759deed9d4a35ce31e', 'context': '올여름 장마가 17일 제주도에서 시작됐다. 서울 등 중부지방은 예년보다 사나흘 정도 늦은 이달 말께 장마가 시작될 전망이다.17일 기상청에 따르면 제주도 남쪽 먼바다에 있는 장마전선의 영향으로 이날 제주도 산간 및 내륙지역에 호우주의보가 내려지면서 곳곳에 100㎜에 육박하는 많은 비가 내렸다. 제주의 장마는 평년보다 2~3일, 지난해보다는 하루 일찍 시작됐다. 장마는 고온다습한 북태평양 기단과 한랭 습윤한 오호츠크해 기단이 만나 형성되는 장마전선에서 내리는 비를 뜻한다.장마전선은 18일 제주도 먼 남쪽 해상으로 내려갔다가 20일께 다시 북상해 전남 남해안까지 영향을 줄 것으로 보인다. 이에 따라 20~21일 남부지방에도 예년보다 사흘 정도 장마가 일찍 찾아올 전망이다. 그러나 장마전선을 밀어올리는 북태평양 고기압 세력이 약해 서울 등 중부지방은 평년보다 사나흘가량 늦은 이달 말부터 장마가 시작될 것이라는 게 기상청의 설명이다. 장마전선은 이후 한 달가량 한반도 중남부를 오르내리며 곳곳에 비를 뿌릴 전망이다. 최근 30년간 평균치에 따르면 중부지방의 장마 시작일은 6월24~25일이었으며 장마기간은 32일, 강수일수는 17.2일이었다.기상청은 올해 장마기간의 평균 강수량이 350~400㎜로 평년과 비슷하거나 적을 것으로 내다봤다. 브라질 월드컵 한국과 러시아의 경기가 열리는 18일 오전 서울은 대체로 구름이 많이 끼지만 비는 오지 않을 것으로 예상돼 거리 응원에는 지장이 없을 전망이다.', 'question': '북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?', 'answers': {'text': '한 달가량', 'answer_start': 478}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, dev_dataset = KoMRC.split(dataset)\n",
        "print(\"Number of Train Samples:\", len(train_dataset))\n",
        "print(\"Number of Dev Samples:\", len(dev_dataset))\n",
        "print(dev_dataset[0])"
      ],
      "metadata": {
        "id": "KbXsR81utkFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1a34111-d8a6-46bd-b37f-ccf1d7ea5256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Train Samples: 10834\n",
            "Number of Dev Samples: 1203\n",
            "{'guid': '844e22ab28924c1697d5ac28801b34c1', 'context': '지난해 주요 연극상을 나눠 가졌던 세 편의 작품이 올봄에 나란히 앙코르 무대를 갖는다. 대한민국연극대상 연기·무대예술상, 동아연극상 작품·희곡·연기상 등을 수상한 ‘알리바이 연대기’(17~20일 대학로 아르코예술극장 대극장, 25일~5월11일 서계동 국립극단 백성희장민호극장), 연극대상에서 대상과 희곡상을 받은 ‘여기가 집이다’(18일~5월22일 대학로 연우소극장), 연극대상 작품·연출상과 김상열연극상 수상작인 ‘황금용’(5월9~18일 서강대 메리홀 대극장)이다. 초연 당시 짧은 상연 기간과 낮은 인지도 등으로 공연을 놓친 연극팬에겐 평단으로부터 작품성을 인정받은 수작을 관람할 수 있는 기회다. ‘알리바이 연대기’는 희곡을 쓰고 연출한 김재엽의 가족사에 근거한 다큐멘터리 드라마다. 1930년에 태어난 한 개인의 사적인 연대기를 바탕으로 그 사이를 파고드는 역사적 순간들을 정밀하게 조명한다. 연출가는 “공적인 권력이 사적인 권리를 지켜주기보다 억압하기 일쑤였던 한국 현대사 속에서 개인은 언제나 무죄를 입증하며 하루하루 자신을 지켜내야 하는 ‘알리바이의 연대기’ 속에서 살아왔다”고 말한다.한국연극평론가협회는 이 작품을 ‘2013년 올해의 연극 베스트3’로 선정하며 “촘촘하고 세세하게 삶에 천착해 개인과 역사에 대한 이분법적 관점을 극복한다. 정치극에 대한 새로운 가능성을 보여줬다”고 평했다. 이 작품으로 연기상을 휩쓴 남명렬을 비롯해 지춘성 정원조 등 초연 배우들이 그대로 출연한다.‘여기가 집이다’는 허름하고 볼품 없는 ‘20년 전통’의 고시원에 모여 사는 사람들의 절망과 희망을 그린 작품. ‘차력사와 아코디언’ ‘택배 왔어요’를 만든 극단 이와삼의 장우재 대표가 직접 대본을 쓰고 연출했다. 나름의 규칙을 가지고 평화로웠던 고시원에 새로운 주인으로 등장한 ‘20세 고등학생’ 동교가 “이제부터 고시원 식구들에게 월세를 받지 않겠다”고 선언하면서 갑작스런 변화의 바람이 분다.날것 그대로의 직설 화법으로 풀어 놓는 풍성한 인생 이야기와 생동감 넘치는 극적 구조로 ‘집’의 본원적 의미와 삶에 대한 성찰의 기회를 제공한다는 평가를 받았다. 재연에서는 중견 배우 김세동이 장씨 역으로 출연해 박무영 김충근 한동규 류제승 김정민 등 초연 배우들과 호흡을 맞춘다.독일 극작가 롤란트 시멜페니히가 쓴 현대극 ‘황금용’은 독일 소도시에 있는 아시아계 간이식당을 배경으로 현대 물질사회와 세계화 속에 가려진 욕망과 폭력, 소외를 그린다. 치통을 앓지만 불법 체류자 신분으로 치과에 가지 못하는 한 젊은 중국인 요리사는 결국 비참한 최후를 맞는다.작품을 연출한 윤광진 용인대 교수는 “극의 배경은 유럽의 한 소도시이지만 서울이나 경기 안산의 어느 거리에서 일어나는 듯 우리에게 가깝게 다가오는 작품”이라며 “지하철에서 마주치는 외국인 근로자들, 그 옆에서 졸고 있는 우리의 이야기”라고 말했다. 이호성 남미정 이동근 한덕호 방현숙 등 초연 배우들이 다시 뭉친다.', 'question': '윤광진 교수가 연출한 연극이 앙코르 공연을 하는 장소는 어디인가?', 'answers': {'text': '서강대 메리홀 대극장', 'answer_start': 246}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.question, self.context, self.answer_start, self.answer_text = self.make_dataset(dataset)\n",
        "\n",
        "    def make_dataset(self, dataset):\n",
        "        context, question, answer_start, answer_text = [], [], [], []\n",
        "        for i, data in enumerate(dataset) :\n",
        "          answer_start.append(data['answers']['answer_start'])\n",
        "          answer_text.append(data['answers']['text'])\n",
        "          context.append(data['context'])\n",
        "          question.append(data['question'])\n",
        "        return question, context, answer_start, answer_text\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.question)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.question[idx], self.context[idx], self.answer_start[idx], self.answer_text[idx]\n"
      ],
      "metadata": {
        "id": "n16XpeEJtkEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_custum = CustomDataset(train_dataset)\n",
        "dev_dataset_custum = CustomDataset(dev_dataset)"
      ],
      "metadata": {
        "id": "8IZUl7A5tzfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch):\n",
        "    global tokenizer\n",
        "    question_list, context_list, answer_start, answer_text, after_list = [], [], [], [], []\n",
        "\n",
        "    for _question, _context, _start, _text in batch:\n",
        "        question_list.append(_question)\n",
        "        context_list.append(_context)\n",
        "        after_list.append(_context[:_start])\n",
        "        answer_start.append(_start)\n",
        "        answer_text.append(_text)\n",
        "\n",
        "    tensorized_input = tokenizer(    \n",
        "        question_list, context_list,\n",
        "        add_special_tokens=True,\n",
        "        padding=\"longest\",  \n",
        "        max_length=512,\n",
        "        truncation=\"only_second\",\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # answer_start token의 위치를 찾기 위해 list로 반환\n",
        "    label_tokens = tokenizer(\n",
        "        question_list, after_list,\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=None\n",
        "    ).input_ids\n",
        "\n",
        "    # answer text token만 변환\n",
        "    answer_tokens = tokenizer(\n",
        "        answer_text,\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=None\n",
        "    ).input_ids\n",
        "\n",
        "    start = []\n",
        "    end = []\n",
        "    for label, answer in zip(label_tokens,answer_tokens):\n",
        "        if len(label) + len(answer) + 1 <= 512:\n",
        "            start.append(len(label) + 2)\n",
        "            end.append(len(label) + len(answer) + 1)\n",
        "        else :\n",
        "            start.append(0)\n",
        "            end.append(0)\n",
        "    start = torch.LongTensor(start)\n",
        "    end = torch.LongTensor(end)\n",
        "    return tensorized_input, start, end, answer_text"
      ],
      "metadata": {
        "id": "MLIBP6TVtzdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataloader(dataset, batch_size, s='train') :\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size =batch_size,\n",
        "      shuffle = True if s == 'train' else False,\n",
        "      collate_fn = custom_collate_fn\n",
        "  )\n",
        "  print(f'batch_size : {batch_size}')\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "rv9JI3mrtzat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"monologg/kobigbird-bert-base\",\n",
        "    use_fast=True # Whether or not to try to load the fast version of the tokenizer.\n",
        "    )"
      ],
      "metadata": {
        "id": "OIycOgUNtzZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "accumulation = 32\n",
        "train_loader = make_dataloader(train_dataset_custum, batch_size//accumulation , 'train')\n",
        "dev_loader = make_dataloader(dev_dataset_custum, batch_size//accumulation,False)"
      ],
      "metadata": {
        "id": "ANL5EJeHtzXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42d4b096-363d-4454-caa6-2c13df2238f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size : 2\n",
            "batch_size : 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained(\"monologg/kobigbird-bert-base\")"
      ],
      "metadata": {
        "id": "eHSBRCzwtzUu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2e5ed82-ce1f-4ca5-ad57-65b326f20f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at monologg/kobigbird-bert-base were not used when initializing BigBirdForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'bert.pooler.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'bert.pooler.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BigBirdForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BigBirdForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BigBirdForQuestionAnswering were not initialized from the model checkpoint at monologg/kobigbird-bert-base and are newly initialized: ['qa_classifier.output.LayerNorm.bias', 'qa_classifier.qa_outputs.bias', 'qa_classifier.output.LayerNorm.weight', 'qa_classifier.output.dense.bias', 'qa_classifier.intermediate.dense.bias', 'qa_classifier.intermediate.dense.weight', 'qa_classifier.output.dense.weight', 'qa_classifier.qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch, start, end, answer_text = next(iter(train_loader))\n",
        "input_ids = batch.input_ids.to(device)\n",
        "attention_mask = batch.attention_mask.to(device)\n",
        "token_type_ids = batch.token_type_ids.to(device)        \n",
        "start = start.to(device)\n",
        "end = end.to(device)\n",
        "# output_logits = torch.stack([outputs.start_logits, outputs.end_logits], dim=2)\n",
        "# print(start,end)"
      ],
      "metadata": {
        "id": "mlT3cfoQtzOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c1e7738-9f73-4163-c1e2-ac8a04a63b0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([216, 685, 182, 115], device='cuda:0') tensor([218, 688, 184, 116], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del batch, input_ids, attention_mask, start, end"
      ],
      "metadata": {
        "id": "J_i1ghReKDln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(input_ids[1][549:551]))\n",
        "print(answer_text[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drmaRAI7PdHJ",
        "outputId": "00833992-b3fe-4800-b3f4-6b583bfe224a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80㎏\n",
            "80㎏\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.to(device)\n",
        "# outputs = model(input_ids = input_ids, attention_mask = attention_mask,start_positions = start, end_positions = end)\n",
        "# start_logits = outputs.start_logits[0].softmax(-1)\n",
        "# end_logits = outputs.end_logits[0].softmax(-1)\n",
        "# probability = torch.triu(start_logits[:, None] @ end_logits[None, :])\n",
        "# print(torch.argmax(probability)//len(end_logits)) # 479\n",
        "# print(torch.argmax(probability)%len(end_logits)) # 493\n",
        "# print(tokenizer.decode(input_ids[0][48:51]))\n",
        "# print(answer_text[0])\n",
        "# print(outputs.loss)"
      ],
      "metadata": {
        "id": "ba6gjMJztkCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs('dump', exist_ok=True)\n",
        "train_losses = []\n",
        "dev_losses = []\n",
        "step = 0\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-8, verbose=True)"
      ],
      "metadata": {
        "id": "rN-q50v40DP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "for epoch in range(3):\n",
        "    print(\"Epoch\", epoch)\n",
        "    # Training\n",
        "    model.train()\n",
        "    running_loss = 0.\n",
        "    losses = []\n",
        "    progress_bar = tqdm(train_loader, desc='Train')\n",
        "    for batch, start, end, answer_text in progress_bar:\n",
        "        input_ids = batch.input_ids.to(device)\n",
        "        attention_mask = batch.attention_mask.to(device)\n",
        "        # token_type_ids = batch.token_type_ids.to(device)        \n",
        "        start = start.to(device)\n",
        "        end = end.to(device)\n",
        "        output = model(input_ids = input_ids, attention_mask = attention_mask,start_positions = start, end_positions = end)\n",
        "\n",
        "        loss = output.loss\n",
        "\n",
        "        (loss / accumulation).backward()\n",
        "        # loss.backward()\n",
        "        running_loss += loss.item()\n",
        "        del batch, input_ids, attention_mask, start, end, loss\n",
        "\n",
        "        step += 1\n",
        "        if step % accumulation:\n",
        "            continue\n",
        "\n",
        "        clip_grad_norm_(model.parameters(), max_norm=1.)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        losses.append(running_loss / accumulation)\n",
        "        running_loss = 0.\n",
        "        progress_bar.set_description(f\"Train - Loss: {losses[-1]:.3f}\")\n",
        "\n",
        "        if step % 512 == 0 :\n",
        "            # Evaluation\n",
        "            dev_loss = []\n",
        "            model.eval()\n",
        "            for batch, start, end, answer_text in dev_loader:\n",
        "\n",
        "                input_ids = batch.input_ids.to(device)\n",
        "                attention_mask = batch.attention_mask.to(device)\n",
        "                # token_type_ids = batch.token_type_ids.to(device)   \n",
        "                start = start.to(device)\n",
        "                end = end.to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(input_ids = input_ids, attention_mask = attention_mask,start_positions = start, end_positions = end)\n",
        "                loss = outputs.loss\n",
        "\n",
        "                dev_loss.append(loss.item())\n",
        "                del batch, input_ids, attention_mask, outputs, loss\n",
        "            # scheduler.step(mean(losses))\n",
        "            dev_losses.append(mean(dev_loss))\n",
        "            print(f\"Evaluation score: {dev_losses[-1]:.3f}\")\n",
        "            scheduler.step(mean(dev_loss))\n",
        "            model.train()\n",
        "    train_losses.append(mean(losses))\n",
        "    print(f\"train score: {train_losses[-1]:.3f}\")\n",
        "\n",
        "    model.save_pretrained(f'dump/model.{epoch}')"
      ],
      "metadata": {
        "id": "GudT5zWU0Gmy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "025a5806-be0c-4fce-d0c8-4abc293850f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:   0%|          | 0/5417 [00:00<?, ?it/s]Attention type 'block_sparse' is not possible if sequence_length: 512 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n",
            "Train - Loss: 3.030:   9%|▉         | 512/5417 [02:47<19:57:28, 14.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation score: 2.451\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train - Loss: 1.865:  19%|█▉        | 1024/5417 [05:33<17:48:08, 14.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation score: 1.752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train - Loss: 1.876:  28%|██▊       | 1536/5417 [08:19<15:43:59, 14.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation score: 1.479\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train - Loss: 1.518:  38%|███▊      | 2048/5417 [11:05<13:41:07, 14.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation score: 1.334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train - Loss: 1.518:  47%|████▋     | 2560/5417 [13:52<11:36:23, 14.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation score: 1.312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train - Loss: 1.435:  57%|█████▋    | 3072/5417 [16:38<9:31:26, 14.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation score: 1.236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train - Loss: 1.600:  66%|██████▌   | 3584/5417 [19:24<7:26:21, 14.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation score: 1.196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train - Loss: 1.310:  70%|██████▉   | 3791/5417 [20:12<06:03,  4.47it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_loader = make_dataloader(dev_dataset_custum, 2,'train')\n",
        "batch, start, end, answer_text = next(iter(dev_loader))\n",
        "input_ids = batch.input_ids.to(device)\n",
        "attention_mask = batch.attention_mask.to(device)\n",
        "# token_type_ids = batch.token_type_ids.to(device)        \n",
        "start = start.to(device)\n",
        "end = end.to(device)\n",
        "print(start, end)"
      ],
      "metadata": {
        "id": "Wja43tjI1dqq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3506460b-ce3c-4040-ca18-a038d36c6984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size : 2\n",
            "tensor([283, 298]) tensor([286, 301])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "outputs = model(input_ids = input_ids, attention_mask = attention_mask,start_positions = start, end_positions = end)\n",
        "start_logits = outputs.start_logits[1].softmax(-1)\n",
        "end_logits = outputs.end_logits[1].softmax(-1)\n",
        "probability = torch.triu(start_logits[:, None] @ end_logits[None, :])\n",
        "print(torch.argmax(probability)//len(end_logits))\n",
        "print(torch.argmax(probability)%len(end_logits)) \n",
        "# print(probability)\n",
        "# print(tokenizer.decode(input_ids[0][94:96]))\n",
        "# print(answer_text[0])"
      ],
      "metadata": {
        "id": "EQEiRkwA1d_R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70ebc179-4f0b-48d7-b8ee-01d58e92b2af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(298)\n",
            "tensor(301)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sINFE6xZGIA",
        "outputId": "2ea61685-fe6c-440d-d800-1c8f4fd46e81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0031, grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ivDUBnZbw7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# For Test"
      ],
      "metadata": {
        "id": "9v2Y8yROby7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained(\"./dump/model.2\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "dKMkAUObtD7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"monologg/kobigbird-bert-base\",\n",
        "    use_fast=True # Whether or not to try to load the fast version of the tokenizer.\n",
        ")"
      ],
      "metadata": {
        "id": "4orEyDFYb7LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple, Dict, Any\n",
        "import json\n",
        "import random\n",
        "\n",
        "class TestKoMRC:\n",
        "    def __init__(self, data, indices: List[Tuple[int, int, int]]):\n",
        "        self._data = data\n",
        "        self._indices = indices\n",
        "\n",
        "    # Json을 불러오는 메소드\n",
        "    @classmethod\n",
        "    def load(cls, file_path: str):\n",
        "        with open(file_path, 'r', encoding='utf-8') as fd:\n",
        "            data = json.load(fd)\n",
        "\n",
        "        indices = []\n",
        "        for d_id, document in enumerate(data['data']):\n",
        "            for p_id, paragraph in enumerate(document['paragraphs']):\n",
        "                for q_id, _ in enumerate(paragraph['qas']):\n",
        "                    indices.append((d_id, p_id, q_id))\n",
        "        \n",
        "        return cls(data, indices)\n",
        "\n",
        "    # 데이터 셋을 잘라내는 메소드\n",
        "    @classmethod\n",
        "    def split(cls, dataset, eval_ratio: float=.1, seed=42):\n",
        "        indices = list(dataset._indices)\n",
        "        random.seed(seed)\n",
        "        random.shuffle(indices)\n",
        "        train_indices = indices[int(len(indices) * eval_ratio):]\n",
        "        eval_indices = indices[:int(len(indices) * eval_ratio)]\n",
        "\n",
        "        return cls(dataset._data, train_indices), cls(dataset._data, eval_indices)\n",
        "\n",
        "    def __getitem__(self, index: int) -> Dict[str, Any]:\n",
        "        d_id, p_id, q_id = self._indices[index]\n",
        "        paragraph = self._data['data'][d_id]['paragraphs'][p_id]\n",
        "\n",
        "        \n",
        "        qa = paragraph['qas'][q_id]\n",
        "\n",
        "        guid = qa['guid']\n",
        "        question = qa['question']\n",
        "        context = paragraph['context']\n",
        "        return {\n",
        "            'guid': guid,\n",
        "            'context': context,\n",
        "            'question': question\n",
        "        }\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self._indices)"
      ],
      "metadata": {
        "id": "bqkxXfgeaIb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TestKoMRC.load('./test.json')\n",
        "print(\"Number of Test Samples\", len(test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN5TM2ltYFzB",
        "outputId": "c84d03aa-8e01-4c0e-d6a4-28fdeb5a98d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Test Samples 4008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(test_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ae7eWBxeZ8_6",
        "outputId": "df1ef072-5ef3-483a-b870-395431f6a9cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'guid': 'd14cb73158624cf094c546d856fd3c80',\n",
              " 'context': 'BMW 코리아(대표 한상윤)는 창립 25주년을 기념하는 ‘BMW 코리아 25주년 에디션’을 한정 출시한다고 밝혔다. 이번 BMW 코리아 25주년 에디션(이하 25주년 에디션)은 BMW 3시리즈와 5시리즈, 7시리즈, 8시리즈 총 4종, 6개 모델로 출시되며, BMW 클래식 모델들로 선보인 바 있는 헤리티지 컬러가 차체에 적용돼 레트로한 느낌과 신구의 조화가 어우러진 차별화된 매력을 자랑한다. 먼저 뉴 320i 및 뉴 320d 25주년 에디션은 트림에 따라 옥스포드 그린(50대 한정) 또는 마카오 블루(50대 한정) 컬러가 적용된다. 럭셔리 라인에 적용되는 옥스포드 그린은 지난 1999년 3세대 3시리즈를 통해 처음 선보인 색상으로 짙은 녹색과 풍부한 펄이 오묘한 조화를 이루는 것이 특징이다. M 스포츠 패키지 트림에 적용되는 마카오 블루는 1988년 2세대 3시리즈를 통해 처음 선보인 바 있으며, 보랏빛 감도는 컬러감이 매력이다. 뉴 520d 25주년 에디션(25대 한정)은 프로즌 브릴리언트 화이트 컬러로 출시된다. BMW가 2011년에 처음 선보인 프로즌 브릴리언트 화이트는 한층 더 환하고 깊은 색감을 자랑하며, 특히 표면을 무광으로 마감해 특별함을 더했다. 뉴 530i 25주년 에디션(25대 한정)은 뉴 3시리즈 25주년 에디션에도 적용된 마카오 블루 컬러가 조합된다. 뉴 740Li 25주년 에디션(7대 한정)에는 말라카이트 그린 다크 색상이 적용된다. 잔잔하면서도 오묘한 깊은 녹색을 발산하는 말라카이트 그린 다크는 장식재로 활용되는 광물 말라카이트에서 유래됐다. 뉴 840i xDrive 그란쿠페 25주년 에디션(8대 한정)은 인도양의 맑고 투명한 에메랄드 빛을 연상케 하는 몰디브 블루 컬러로 출시된다. 특히 몰디브 블루는 지난 1993년 1세대 8시리즈에 처음으로 적용되었던 만큼 이를 오마주하는 의미를 담고 있다.',\n",
              " 'question': '말라카이트에서 나온 색깔을 사용한 에디션은?'}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.question, self.context, self.guid= self.make_dataset(dataset)\n",
        "\n",
        "    def make_dataset(self, dataset):\n",
        "        context, question, guid = [], [], []\n",
        "        for i, data in enumerate(dataset) :\n",
        "          context.append(data['context'])\n",
        "          question.append(data['question'])\n",
        "          guid.append(data['guid'])\n",
        "        return question, context, guid\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.question)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.question[idx], self.context[idx] , self.guid[idx]"
      ],
      "metadata": {
        "id": "46YSGtldZy0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset_custum = TestDataset(test_dataset)"
      ],
      "metadata": {
        "id": "0ZfTkP7paeBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_collate_fn(batch):\n",
        "    global tokenizer\n",
        "    question_list, context_list, guid_list= [], [], []\n",
        "\n",
        "    for _question, _context, _guid in batch:\n",
        "        question_list.append(_question)\n",
        "        context_list.append(_context)\n",
        "        guid_list.append(_guid)\n",
        "\n",
        "    tensorized_input = tokenizer(    \n",
        "        question_list, context_list,\n",
        "        add_special_tokens=True,\n",
        "        padding=\"longest\",  \n",
        "        max_length=2024,\n",
        "        truncation=\"only_second\",\n",
        "        return_offsets_mapping=True,\n",
        "        return_tensors='pt'\n",
        "        \n",
        "    )\n",
        "\n",
        "    return tensorized_input, guid_list, context_list"
      ],
      "metadata": {
        "id": "0ZffOTZyanJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(test_dataset_custum, batch_size = 2, collate_fn = test_collate_fn)"
      ],
      "metadata": {
        "id": "rZsyrotnbGAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NZkG8WLvbir-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input, guid = next(iter(test_loader))\n",
        "input_ids = input.input_ids.to(device)\n",
        "attention_mask = input.attention_mask.to(device)"
      ],
      "metadata": {
        "id": "UaoPAbu9h1--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "start_logits = outputs.start_logits[0].softmax(-1)\n",
        "end_logits = outputs.end_logits[0].softmax(-1)\n",
        "probability = torch.triu(start_logits[:, None] @ end_logits[None, :])\n",
        "start_token = torch.argmax(probability)//len(end_logits)\n",
        "end_token = torch.argmax(probability)%len(end_logits)\n",
        "# print(probability)\n",
        "print(tokenizer.decode(input_ids[0][start_token:end_token + 1]))\n",
        "# print(answer_text[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPGUALV9h4JX",
        "outputId": "a47a5131-894b-4c36-a927-dc5fd5b3d9e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "뉴 740Li 25주년 에디션 ( 7대 한정 )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(input_ids[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zHruwOyiKQn",
        "outputId": "faabc91f-cf98-476e-9c7f-c3753741657b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS] 말라카이트에서 나온 색깔을 사용한 에디션은? [SEP] BMW 코리아 ( 대표 한상윤 ) 는 창립 25주년을 기념하는 ‘ BMW 코리아 25주년 에디션 ’ 을 한정 출시한다고 밝혔다. 이번 BMW 코리아 25주년 에디션 ( 이하 25주년 에디션 ) 은 BMW 3시리즈와 5시리즈, 7시리즈, 8시리즈 총 4종, 6개 모델로 출시되며, BMW 클래식 모델들로 선보인 바 있는 헤리티지 컬러가 차체에 적용돼 레트로한 느낌과 신구의 조화가 어우러진 차별화된 매력을 자랑한다. 먼저 뉴 320i 및 뉴 320d 25주년 에디션은 트림에 따라 옥스포드 그린 ( 50대 한정 ) 또는 마카오 블루 ( 50대 한정 ) 컬러가 적용된다. 럭셔리 라인에 적용되는 옥스포드 그린은 지난 1999년 3세대 3시리즈를 통해 처음 선보인 색상으로 짙은 녹색과 풍부한 펄이 오묘한 조화를 이루는 것이 특징이다. M 스포츠 패키지 트림에 적용되는 마카오 블루는 1988년 2세대 3시리즈를 통해 처음 선보인 바 있으며, 보랏빛 감도는 컬러감이 매력이다. 뉴 520d 25주년 에디션 ( 25대 한정 ) 은 프로즌 브릴리언트 화이트 컬러로 출시된다. BMW가 2011년에 처음 선보인 프로즌 브릴리언트 화이트는 한층 더 환하고 깊은 색감을 자랑하며, 특히 표면을 무광으로 마감해 특별함을 더했다. 뉴 530i 25주년 에디션 ( 25대 한정 ) 은 뉴 3시리즈 25주년 에디션에도 적용된 마카오 블루 컬러가 조합된다. 뉴 740Li 25주년 에디션 ( 7대 한정 ) 에는 말라카이트 그린 다크 색상이 적용된다. 잔잔하면서도 오묘한 깊은 녹색을 발산하는 말라카이트 그린 다크는 장식재로 활용되는 광물 말라카이트에서 유래됐다. 뉴 840i xDrive 그란쿠페 25주년 에디션 ( 8대 한정 ) 은 인도양의 맑고 투명한 에메랄드 빛을 연상케 하는 몰디브 블루 컬러로 출시된다. 특히 몰디브 블루는 지난 1993년 1세대 8시리즈에 처음으로 적용되었던 만큼 이를 오마주하는 의미를 담고 있다. [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "guid_list = []\n",
        "infer_list = []\n",
        "for i,(batch, guid, context) in tqdm(enumerate(test_loader)):\n",
        "    input_ids = batch.input_ids.to(device)\n",
        "    attention_mask = batch.attention_mask.to(device)\n",
        "    offset_mapping = batch.offset_mapping.to(device)\n",
        "\n",
        "    outputs = model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "    start_logits = outputs.start_logits.softmax(-1)\n",
        "    end_logits = outputs.end_logits.softmax(-1)\n",
        "    \n",
        "    for guid, start, end, input_ids, offset_mapping, context in zip(guid, start_logits, end_logits, input_ids, offset_mapping, context):\n",
        "        proba = torch.triu(start[:, None] @ end[None,:])\n",
        "        start = torch.argmax(proba)//len(end)\n",
        "        end = torch.argmax(proba)%len(end)\n",
        "        inference = tokenizer.decode(input_ids[start:end + 1])\n",
        "        start_charactor = offset_mapping[start][0]\n",
        "        end_charactor = offset_mapping[end][1]\n",
        "        inference = context[start_charactor:end_charactor]\n",
        "        if len(inference) > 30 :\n",
        "            infer_list.append('무무무무무무')\n",
        "        else:\n",
        "            infer_list.append(inference)\n",
        "        guid_list.append(guid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knkIfCGmYWwY",
        "outputId": "81be6e8f-bb3f-409a-eb1e-b38957535fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2004it [04:12,  7.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset_custum[7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1txtJEQnzVDl",
        "outputId": "2be41ab4-df7c-473c-fbd0-5bcd869cd07d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('필리핀 숙박업소가 손님을 다시 받으려면 필수적으로 떼야 하는 서류는?',\n",
              " '두테르테 필리핀 대통령이 세부를 제외한 전 지역에서 일반 지역사회 격리 조치 (GCQ) 혹은 수정된 일반 지역사회 격리 조치 (MGCQ)를 7월 15일까지 연장함에 따라, 필리핀 관광부는 코로나 예방을 위해, 외식, 교통, 숙박 등 산업 전반에 걸쳐 안전 수칙을 마련하여 실시하고 있다고 밝혔다. 숙박시설의 경우, 필리핀 관광부는 영업을 재개하려는 호텔 및 기타 숙박업소에 의무적으로 ‘운영허가 인증서’를 발급 받도록 하여, 관광객들이 안전하게 숙박시설을 이용할 수 있도록 조치하고 있다. 인증서를 받은 숙박시설에서는 마스크 미 착용시, 체크인이 불가하며, 비 접촉식 체온 확인 후 입실이 가능하다. 또한 로비, 엘리베이터 바닥 등에 1m 간격으로 마크를 부착하여, 사회적 거리 두기를 준수할 수 있게 했다. 그 외에도 객실 앞으로 식사 배달, 전 숙박시설 소독, 직원들의 마스크 착용 의무화 등의 조치로 안전한 휴식을 돕고 있다. 식당 매장 운영은 전체 50%의 가동률로 오후 9시까지 영업이 가능하며, 고객과 직원 간 일정한 거리를 유지해야 한다. 또한 테이블 당 거리가 1m를 유지해야 하며, 뷔페 레스토랑의 경우, 원하는 음식을 주문서에 표시한 뒤, 직원이 접시에 담아 음식을 가져다 주는 형식으로 운영된다. 공공의 위생을 위해, 직원에게 직접 현금을 건네는 접촉 결제는 불가하다. 지역사회 격리조치 기간 동안 대중 교통수단은 점진적으로 허용되고 있다. 이에 따라, 필리핀 관광부는 모든 대중교통 탑승객들에게 마스크를 착용하고 사회적 거리 두기를 준수할 것을 강조했다. 모든 교통수단은 전체 탑승 가능 승객수의 50%만 탑승이 가능하다. 또한 관광객 전용 교통수단의 경우에는, 기본 위생 키트 및 비접촉식 체온계를 구비해야 한다. 현재 외국인들의 필리핀 입국은 외교관 비자 소지자, 필리핀 국적의 외국인 배우자 등으로 제한되고 있으며, 입국 시 시스템에 개인 정보를 기입한 후, 생성 된 QR 코드를 검역 데스크에 제출하고 코로나 진단 검사를 받아야 한다. 검사 후, 호텔로 이동하여 결과가 나올 때까지 자가격리를 진행해야 하며 검사결과는 72시간 내로 받아볼 수 있다. 필리핀 관광부는 전 세계적으로 어려운 시기에, 안전을 우선적으로 고려하여 가이드라인을 마련하고 있는 만큼 정부 지침에 적극적으로 따라줄 것을 요청했다. 또한 필리핀 관광부에서 진행하고 있는 디지털 온라인 캠페인 등으로 필리핀으로 언택트 여행을 떠나며, 다음 여행에 최우선적으로 필리핀을 선택해주길 당부했다.',\n",
              " '5c19b9781f8a4f0faa9045274c38a8b4')"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(infer_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Eu9WaEQfxqX",
        "outputId": "8d77aaa9-2315-45e5-f3bb-212753dfbdad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4008"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WNVN2jgC_A7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({'Id' : guid_list,'Predicted' : infer_list})"
      ],
      "metadata": {
        "id": "PYxbwRHUdgJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "5fs8H3AC_OcS",
        "outputId": "d9771b8a-03de-4c18-a996-82c5b7d9e649"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                    Id         Predicted\n",
              "0     d14cb73158624cf094c546d856fd3c80  뉴 740Li 25주년 에디션\n",
              "1     906631384e91493ebe1c7f34aea6f241         독일 뒤셀도르프로\n",
              "2     35e61dcb479643448a2cb7d326ae50a6              링크트인\n",
              "3     075e761b370040cb9041eecd39afc27c        링크트인과 페이스북\n",
              "4     e67ed38f3dd944be94d5b4c53731f334              마드리드\n",
              "...                                ...               ...\n",
              "4003  05fcb8054dc44dab8683579c2cf5e465             200만엔\n",
              "4004  cc7f826b66724ce9b39e3a974ca15661          중동 건설 현장\n",
              "4005  3282034aa41e4fab980851ffd4a868dd         아시아~유럽 노선\n",
              "4006  0a73550b36df4baf82ac2f98619d22e7               10일\n",
              "4007  dfe6ef25f84f461e89e54d370e4521d5               벨기에\n",
              "\n",
              "[4008 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7084d6f1-5f3f-489a-946e-d783b423c2df\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>d14cb73158624cf094c546d856fd3c80</td>\n",
              "      <td>뉴 740Li 25주년 에디션</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>906631384e91493ebe1c7f34aea6f241</td>\n",
              "      <td>독일 뒤셀도르프로</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>35e61dcb479643448a2cb7d326ae50a6</td>\n",
              "      <td>링크트인</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>075e761b370040cb9041eecd39afc27c</td>\n",
              "      <td>링크트인과 페이스북</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>e67ed38f3dd944be94d5b4c53731f334</td>\n",
              "      <td>마드리드</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4003</th>\n",
              "      <td>05fcb8054dc44dab8683579c2cf5e465</td>\n",
              "      <td>200만엔</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4004</th>\n",
              "      <td>cc7f826b66724ce9b39e3a974ca15661</td>\n",
              "      <td>중동 건설 현장</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4005</th>\n",
              "      <td>3282034aa41e4fab980851ffd4a868dd</td>\n",
              "      <td>아시아~유럽 노선</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4006</th>\n",
              "      <td>0a73550b36df4baf82ac2f98619d22e7</td>\n",
              "      <td>10일</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4007</th>\n",
              "      <td>dfe6ef25f84f461e89e54d370e4521d5</td>\n",
              "      <td>벨기에</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4008 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7084d6f1-5f3f-489a-946e-d783b423c2df')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7084d6f1-5f3f-489a-946e-d783b423c2df button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7084d6f1-5f3f-489a-946e-d783b423c2df');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('/content/drive/MyDrive/Colab Notebooks/[GOORM]NLP/Project/Machine Reading Comprehension/KoBigbird_base.csv',encoding = 'utf8')"
      ],
      "metadata": {
        "id": "v6xwNfNudRX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jNs5YPOF7qiT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}